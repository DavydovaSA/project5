---
title: "Упражнение 5"
author: "Davydova S.A."
date: '15 апреля 2017 г '
output: html_document
---

```{r Данные, include = F}
# загрузка пакетов
library('ISLR')              # набор данных Auto
library('GGally')            # матричные графики
library('boot')

head(Auto)
my.seed <- 1

# Метод проверочной выборки ====================================================

# общее число наблюдений
n <- nrow(Auto)

# доля обучающей выборки
train.percent <- 0.5

# выбрать наблюдения в обучающую выборку
set.seed(my.seed)
inTrain <- sample(n, n * train.percent)
inTrain

```

# Метод проверочной выборки

```{r echo = T, message = F, warning = F}
# описательные статистики по переменным
attach(Auto)
# подгонка линейной модели на обучающей выборке
fit.lm.1 <- lm(mpg ~ weight + acceleration + year + cylinders, 
               subset = inTrain)
# считаем MSE на тестовой выборке
mean((mpg[-inTrain] - predict(fit.lm.1,
                              Auto[-inTrain, ]))^2)

# подгонка линейной модели на обучающей выборке
fit.lm.2 <- lm(mpg ~ weight + acceleration + year, 
               subset = inTrain)
# считаем MSE на тестовой выборке
mean((mpg[-inTrain] - predict(fit.lm.2,
                              Auto[-inTrain, ]))^2)
# отсоединить таблицу с данными
detach(Auto)

```

```{r echo = T, warning = F, error = F}

# подгонка линейной модели на обучающей выборке
fit.glm.1 <- glm(mpg ~ weight + acceleration + year + cylinders, data = Auto)
# считаем LOOCV-ошибку
cv.err.1 <- cv.glm(Auto, fit.glm.1)
# результат: первое число -- по формуле LOOCV-ошибки,
#  второе -- с поправкой на смещение
cv.err.1$delta[1]

fit.glm.2 <- glm(mpg ~ weight + acceleration + year, data = Auto)
# считаем LOOCV-ошибку
cv.err.2 <- cv.glm(Auto, fit.glm.2)
# результат: первое число -- по формуле LOOCV-ошибки,
#  второе -- с поправкой на смещение
cv.err.2$delta[1]

``` 

# Перекрёстная проверка по отдельным наблюдениям (LOOCV)

```{r echo = T, warning = F, error = F}

cv.err.k.fold.1 <- cv.glm(Auto, fit.glm.1,
                          K = 5)$delta[1]
cv.err.k.fold.2 <- cv.glm(Auto, fit.glm.1,
                             K = 10)$delta[1]
cv.err.k.fold.1
cv.err.k.fold.2


cv.err.k.fold.3 <- cv.glm(Auto, fit.glm.2,
                            K = 5)$delta[1]
cv.err.k.fold.4 <- cv.glm(Auto, fit.glm.2,
                             K = 10)$delta[1]
cv.err.k.fold.3
cv.err.k.fold.4

```

Модель, построенная только на непрерывных объясняющих переменных, является лучшей по минимуму ошибки.

# Бутстреп

```{r echo = T, warning = F, error = F}

boot.fn <- function(data, index){
  coef(lm(mpg ~ weight + acceleration + year + cylinders, data = data, subset = index))
}
boot.fn(Auto, 1:n)

boot.fn.2 <- function(data, index){
  coef(lm(mpg ~ weight + acceleration + year, data = data, subset = index))
}
boot.fn.2(Auto, 1:n)


boot(Auto, boot.fn, 1000)
boot(Auto, boot.fn.2, 1000)

# сравнение с МНК

summary(lm(mpg ~ weight + acceleration + year + cylinders, data = Auto))$coef

# для модели только с непрерывными объясняющими переменными:
summary(lm(mpg ~ weight + acceleration + year, data = Auto))$coef

```

Оценки отличаются, т.к. МНК - параметрический метод с допущениями.
